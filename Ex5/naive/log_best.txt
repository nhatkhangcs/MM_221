Epoch: 1000, train loss: 1.7013315, test loss: 1.4149590
Epoch: 2000, train loss: 1.0879632, test loss: 0.8812754
Epoch: 3000, train loss: 1.0248492, test loss: 0.8483872
Epoch: 4000, train loss: 0.9941214, test loss: 0.8407990
Epoch: 5000, train loss: 0.9800303, test loss: 0.8458886
Epoch: 6000, train loss: 0.9705737, test loss: 0.8422338
Epoch: 7000, train loss: 0.9637191, test loss: 0.8347459
Epoch: 8000, train loss: 0.9595532, test loss: 0.8287681
Epoch: 9000, train loss: 0.9576766, test loss: 0.8264511
Epoch: 10000, train loss: 0.9610669, test loss: 0.8383215
Epoch: 11000, train loss: 0.9566970, test loss: 0.8249957
Epoch: 12000, train loss: 0.9565095, test loss: 0.8239750
Epoch: 13000, train loss: 0.9595160, test loss: 0.8205696
Epoch: 14000, train loss: 0.9566484, test loss: 0.8210573
Epoch: 15000, train loss: 0.9567747, test loss: 0.8202810
Epoch: 16000, train loss: 0.9559237, test loss: 0.8220639
Epoch: 17000, train loss: 0.9560138, test loss: 0.8214949
Epoch: 18000, train loss: 0.9556738, test loss: 0.8205703
Epoch: 19000, train loss: 0.9556595, test loss: 0.8195646
Epoch: 20000, train loss: 0.9554437, test loss: 0.8208107
Epoch: 21000, train loss: 0.9553354, test loss: 0.8199292
Epoch: 22000, train loss: 0.9552432, test loss: 0.8196952
Epoch: 23000, train loss: 0.9551739, test loss: 0.8193878
Epoch: 24000, train loss: 0.9551349, test loss: 0.8188359
Epoch: 25000, train loss: 0.9550683, test loss: 0.8195178
Epoch: 26000, train loss: 0.9557922, test loss: 0.8190269
Epoch: 27000, train loss: 0.9551122, test loss: 0.8198514
Epoch: 28000, train loss: 0.9557726, test loss: 0.8156283
Epoch: 29000, train loss: 0.9551439, test loss: 0.8168595
Epoch: 30000, train loss: 0.9563113, test loss: 0.8158681
Epoch: 31000, train loss: 0.9549311, test loss: 0.8178731
Epoch: 32000, train loss: 0.9548709, test loss: 0.8183140
Epoch: 33000, train loss: 0.9548518, test loss: 0.8182949
Epoch: 34000, train loss: 0.9548491, test loss: 0.8178064
Epoch: 35000, train loss: 0.9548258, test loss: 0.8188407
Epoch: 36000, train loss: 0.9548017, test loss: 0.8181562
Epoch: 37000, train loss: 0.9569427, test loss: 0.8153071
Epoch: 38000, train loss: 0.9547719, test loss: 0.8180257
Epoch: 39000, train loss: 0.9556230, test loss: 0.8193302
Epoch: 40000, train loss: 0.9547458, test loss: 0.8179055
R0 = -2.0000, J0 =  2.9990
Model loaded from "fit/netfit_softplus_1.pt"
Epoch: 500, train loss: 2892.4421387, test loss: 1612.2578125
Epoch: 1000, train loss: 1699.8962402, test loss: 961.7484131
Epoch: 1500, train loss: 924.0778809, test loss: 535.5223389
Epoch: 2000, train loss: 456.3825073, test loss: 275.6676636
Epoch: 2500, train loss: 202.6516571, test loss: 132.0163116
Epoch: 3000, train loss: 83.2412491, test loss: 62.1151161
Epoch: 3500, train loss: 36.6217041, test loss: 33.0456352
Epoch: 4000, train loss: 22.3481026, test loss: 22.9325485
Epoch: 4500, train loss: 19.1216259, test loss: 19.9368343
Epoch: 5000, train loss: 18.5224705, test loss: 19.0512371
Epoch: 5500, train loss: 18.2755146, test loss: 18.6582146
Epoch: 6000, train loss: 18.0036316, test loss: 18.3271942
Epoch: 6500, train loss: 17.6630383, test loss: 17.9417381
Epoch: 7000, train loss: 17.2374516, test loss: 17.4637527
Epoch: 7500, train loss: 16.7101860, test loss: 16.8718166
Epoch: 8000, train loss: 16.0641766, test loss: 16.1465778
Epoch: 8500, train loss: 15.2840595, test loss: 15.2708101
Epoch: 9000, train loss: 14.3594770, test loss: 14.2329454
Epoch: 9500, train loss: 13.2894983, test loss: 13.0319691
Epoch: 10000, train loss: 12.0875473, test loss: 11.6830587
Epoch: 10500, train loss: 10.7849379, test loss: 10.2215033
Epoch: 11000, train loss: 9.4309502, test loss: 8.7027864
Epoch: 11500, train loss: 8.0876446, test loss: 7.1967201
Epoch: 12000, train loss: 6.8203912, test loss: 5.7767706
Epoch: 12500, train loss: 5.6871200, test loss: 4.5080028
Epoch: 13000, train loss: 4.7300649, test loss: 3.4377317
Epoch: 13500, train loss: 3.9713206, test loss: 2.5905797
Epoch: 14000, train loss: 3.4125161, test loss: 1.9681758
Epoch: 14500, train loss: 3.0365076, test loss: 1.5510058
Epoch: 15000, train loss: 2.8111925, test loss: 1.3027284
Epoch: 15500, train loss: 2.6953917, test loss: 1.1767930
Epoch: 16000, train loss: 2.6469851, test loss: 1.1256299
Epoch: 16500, train loss: 2.6316559, test loss: 1.1105686
Epoch: 17000, train loss: 2.6283021, test loss: 1.1079845
Epoch: 17500, train loss: 2.6278503, test loss: 1.1079817
Epoch: 18000, train loss: 2.6278174, test loss: 1.1081027
Epoch: 18500, train loss: 2.6278152, test loss: 1.1081316
Epoch: 19000, train loss: 2.6278162, test loss: 1.1081357
Epoch: 19500, train loss: 2.6278157, test loss: 1.1081345
Epoch: 20000, train loss: 2.6278157, test loss: 1.1081343
Epoch: 20500, train loss: 2.6278164, test loss: 1.1081382
Epoch: 21000, train loss: 2.6278167, test loss: 1.1081353
Epoch: 21500, train loss: 2.6278164, test loss: 1.1081400
Epoch: 22000, train loss: 2.6278148, test loss: 1.1081378
Epoch: 22500, train loss: 2.6278157, test loss: 1.1081398
Epoch: 23000, train loss: 2.6278162, test loss: 1.1081420
Epoch: 23500, train loss: 2.6278150, test loss: 1.1081307
Epoch: 24000, train loss: 2.6278164, test loss: 1.1081773
Epoch: 24500, train loss: 2.6278155, test loss: 1.1083078
Epoch: 25000, train loss: 2.6278169, test loss: 1.1078005
Epoch: 25500, train loss: 2.6278150, test loss: 1.1091615
Epoch: 26000, train loss: 2.6278164, test loss: 1.1081203
Epoch: 26500, train loss: 2.6278143, test loss: 1.1081526
Epoch: 27000, train loss: 2.6278152, test loss: 1.1081589
Epoch: 27500, train loss: 2.6278150, test loss: 1.1081426
Epoch: 28000, train loss: 2.6278164, test loss: 1.1082110
Epoch: 28500, train loss: 2.6278152, test loss: 1.1087449
Epoch: 29000, train loss: 2.6278164, test loss: 1.1085850
Epoch: 29500, train loss: 2.6278157, test loss: 1.1080465
Epoch: 30000, train loss: 2.6278207, test loss: 1.1094760
Epoch: 30500, train loss: 2.6278155, test loss: 1.1077521
Epoch: 31000, train loss: 2.6278172, test loss: 1.1072316
Epoch: 31500, train loss: 2.6278193, test loss: 1.1060925
Epoch: 32000, train loss: 2.6278152, test loss: 1.1080520
Epoch: 32500, train loss: 2.6278157, test loss: 1.1080641
Epoch: 33000, train loss: 2.6278164, test loss: 1.1081434
Epoch: 33500, train loss: 2.6278155, test loss: 1.1081578
Epoch: 34000, train loss: 2.6278155, test loss: 1.1079067
Epoch: 34500, train loss: 2.6278145, test loss: 1.1083231
Epoch: 35000, train loss: 2.6278203, test loss: 1.1097267
Epoch: 35500, train loss: 2.6278150, test loss: 1.1079645
Epoch: 36000, train loss: 2.6278176, test loss: 1.1076843
Epoch: 36500, train loss: 2.6278167, test loss: 1.1090918
Epoch: 37000, train loss: 2.6278152, test loss: 1.1075917
Epoch: 37500, train loss: 2.6278155, test loss: 1.1081370
Epoch: 38000, train loss: 2.6278150, test loss: 1.1082753
Epoch: 38500, train loss: 2.6278169, test loss: 1.1081324
Epoch: 39000, train loss: 2.6278155, test loss: 1.1082381
Epoch: 39500, train loss: 2.6278164, test loss: 1.1082145
Epoch: 40000, train loss: 2.6278157, test loss: 1.1082506
a = 2.1601, b = 3.7439, c = 5.6771, d = -2.5837